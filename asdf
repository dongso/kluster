#!/usr/bin/env python3
import os
import glob

import cv2
import numpy as np
from keras.models import Model
from keras.layers import Dense, Activation, MaxPool2D, Conv2D, Flatten, Dropout, Input, BatchNormalization, Add
from keras.optimizers import Adam


# Worker function for custom model
def conv_block(x, filters):
    x = BatchNormalization() (x)        #Batch Normalization은 딥러닝 네트웍의 학습에서 gradient vanishing과 exploding을 회피하기 위한 방법 중의 하나이다.
    x = Conv2D(filters, (3, 3), activation='relu', padding='same') (x)     #필터로 특징을 뽑아주는 컨볼루션(Convolution) 레이어
    #    컨볼루션 필터의 수 , 컨볼루션 커널의 (행, 열) , 활성화 함수 설정(rectifier 함수, 은익층에 주로 사용),
    #    activation : 활성화 함수 설정
    #       ‘linear’ : 디폴트 값, 입력뉴런과 가중치로 계산된 결과값이 그대로 출력
    #       ‘relu’ : rectifier 함수, 은익층에 주로 사용
    #       ‘sigmoid’ : 시그모이드 함수, 이진 분류 문제에서 출력층에 주로 사용
    #       ‘softmax’ : 소프트맥스 함수, 다중 클래스 분류 문제에서 출력층에 사용
    #     padding : 경계 처리 방법을 정의, same=출력 이미지 사이즈가 입력 이미지 사이즈와 동일  , vaild=유효한 영역만 출력
    
    x = BatchNormalization() (x)
    shortcut = x
    x = Conv2D(filters, (3, 3), activation='relu', padding='same') (x)
    x = Add() ([x, shortcut])
    x = MaxPool2D((2, 2), strides=(2, 2)) (x)       #사소한 변화를 무시해주는 맥스풀링(Max Pooling) 레이어
                                                    #(2,2)출력 영상 크기는 입력 영상 크기의 반으로

    return x

# DIY model for training (instead of using standard model package)
def custom_model(input_shape, n_classes):

    input_tensor = Input(shape=input_shape)

    x = conv_block(input_tensor, 32)
    x = conv_block(x, 64)
    x = conv_block(x, 128)
    x = conv_block(x, 256)
    x = conv_block(x, 512)

    x = Flatten() (x)       # 영상을 일차원으로 바꿔주는 플래튼(Flatten) 레이어
    x = BatchNormalization() (x)
    x = Dense(512, activation='relu') (x)   #입출력을 모두 연결해주는 Dense 레이어, 512개 뉴런 출력
    x = Dense(512, activation='relu') (x)

    output_layer = Dense(n_classes, activation='softmax') (x)

    inputs = [input_tensor]
    model = Model(inputs, output_layer)

    return model


# main loop
def main():

    # Data parameter
    input_height = 48
    input_width = 48

    input_channel = 3
    input_shape = (input_height, input_width, input_channel)
    n_classes = 3   #low, high, stop sign 인식

    # Modeling
    # 'custom':
    model = custom_model(input_shape, n_classes)

    adam = Adam()
    model.compile(
        optimizer=adam,
        loss='categorical_crossentropy',
        metrics=['acc'],
    )

    # Search all images
    data_dir = 'C:\TestAI\AIfan\Data'
    match_obj1 = os.path.join('c:\\', 'TestAI', 'AIfan', 'Data', 'sleeping', '*.jpg')
    paths_obj1 = glob.glob(match_obj1)

    match_obj2 = os.path.join('c:\\', 'TestAI', 'AIfan', 'Data', 'not sleeping', '*.jpg')
    paths_obj2 = glob.glob(match_obj2)

    match_test = os.path.join('c:\\', 'TestAI', 'AIfan', 'Data', 'test', '*.jpg')
    paths_test = glob.glob(match_test)

    n_train = len(paths_obj1) + len(paths_obj2)
    n_test = len(paths_test)

    # Initialization dataset matrix
    trainset = np.zeros(
        shape=(n_train, input_height, input_width, input_channel),
        dtype='float32',
    )
    label = np.zeros(
        shape=(n_train, n_classes),
        dtype='float32',
    )
    testset = np.zeros(
        shape=(n_test, input_height, input_width, input_channel),
        dtype='float32',
    )

    # Read image and resize to data set
    paths_train = paths_obj1 + paths_obj2

    for ind, path in enumerate(paths_train):
        try:
            image = cv2.imread(path)
            resized_image = cv2.resize(image, (input_width, input_height))
            trainset[ind] = resized_image

        except Exception as e:
            print(path) # print out the Image that cause exception error

    for ind, path in enumerate(paths_test):
        try:
            image = cv2.imread(path)
            resized_image = cv2.resize(image, (input_width, input_height))
            testset[ind] = resized_image

        except Exception as e:
            print(path) # print out the Image that cause exception error

    # Set the mark of the training set
    n_obj1 = len(paths_obj1)
    n_obj2 = len(paths_obj2)

    begin_ind = 0
    end_ind = n_obj1
    label[begin_ind:end_ind, 0] = 1.0

    begin_ind = n_obj1
    end_ind = n_obj1 + n_obj2
    label[begin_ind:end_ind, 1] = 1.0

    # Normalize the value between 0 and 1
    trainset = trainset / 255.0
    testset = testset / 255.0

    # Training model
    model.fit(
        trainset,
        label,
        epochs=8,  # no. of rounds of training => 8 rounds
        validation_split=0.2,   # percentage of dataset use for validation at trainiing => 20% (2000 images, 1600 for training, 400 for validation)
    )

    # Saving model architecture and weights (parameters)
    model_desc = model.to_json()
    model_file = 'C:/TestAI/AIfan/Data1/model.json'
    with open(model_file, 'w') as file_model:
        file_model.write(model_desc)

    weights_file = 'C:/TestAI/AIfan/Data1/weights.h5'
    model.save_weights(weights_file)

    # Execution predication
    if testset.shape[0] != 0:
        result_onehot = model.predict(testset)
        result_sparse = np.argmax(result_onehot, axis=1)
    else:
        result_sparse = list()

    # Print predication results
    print('File name\t forecast category')

    for path, label_id in zip(paths_test, result_sparse):
      filename = os.path.basename(path)
      if label_id == 0:
          label_name = 'sleeping'
      elif label_id == 1:
          label_name = 'not sleeping'

      print('%s\t%s' % (filename, label_name))

if __name__ == '__main__':
    main()
